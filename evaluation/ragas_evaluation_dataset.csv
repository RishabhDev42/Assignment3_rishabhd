id,question,answer
1,What are the three main evaluation metrics used by ARES?,"ARES evaluates RAG systems based on context relevance, answer faithfulness, and answer relevance."
2,What type of model does ARES fine-tune to act as evaluation judges?,"ARES fine-tunes DeBERTa-v3-Large models as LLM judges for context relevance, answer faithfulness, and answer relevance."
3,What is the role of Prediction-Powered Inference in ARES?,Prediction-Powered Inference provides confidence intervals for RAG evaluation scores using a small human validation set and predictions from the LLM judges.
4,Who introduced the concept of Knowledge Distillation?,Knowledge Distillation was formally introduced by Hinton et al. in 2015.
5,What is the main goal of Knowledge Distillation?,The goal of Knowledge Distillation is to transfer knowledge from a large teacher model to a smaller student model while maintaining performance.
6,What are the three primary types of Knowledge Distillation?,"The three main types are response-based, feature-based, and relation-based distillation."
7,What is feature-based knowledge distillation?,Feature-based distillation transfers intermediate representations or activation maps from the teacher to the student model.
8,What is the role of an activation function in a neural network?,An activation function decides whether a neuron should be activated and introduces non-linearity into the neural network.
9,What happens if a neural network has no activation function?,"Without activation functions, the network becomes purely linear and cannot learn complex relationships."
10,Why is the tanh activation function preferred over the sigmoid in hidden layers?,"Tanh is preferred because it is zero-centered, allowing easier mapping of output values and faster convergence."
11,What are the two primary components of a RAG system and what are their four corresponding phases?","The two primary components are Retrieval and Generation. The four corresponding phases are indexing, search, prompting, and inferencing."
12,"The Auepora framework defines five core evaluation targets for the retrieval and generation components. What are they?","The five core targets are: Relevance and Accuracy for the Retrieval component, and Relevance, Faithfulness, and Correctness for the Generation component."
13,"What is the mathematical formula for Mean Reciprocal Rank (MRR), and what do the variables represent?","The formula is $MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{rank_{i}}$. In this formula, $|Q|$ represents the number of queries, and $rank_{i}$ is the rank position of the first relevant document for the i-th query."
14,"The RAG evaluation survey discusses several 'Additional Requirements' for evaluating RAG systems beyond core retrieval and generation. Name four of these requirements.","Four additional requirements mentioned are Latency, Diversity, Noise Robustness, and Negative Rejection."
15,"What is the key difference between Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)?","Post-Training Quantization (PTQ) is applied after a model has been trained and does not require retraining. In contrast, Quantization-Aware Training (QAT) incorporates the quantization process directly into the model's training phase to improve accuracy."
16,"How does the 'Comprehensive Survey of Small Language Models' propose to formally define a Small Language Model (SLM)?","The survey defines an SLM as a model falling within a range where the 'lower bound is the minimum size at which the model exhibits emergent abilities for a specialized task, and the upper bound is the largest size manageable within limited resource conditions'."
17,"What are the three primary techniques for obtaining smaller models from larger ones?","The three primary techniques are pruning (removing less critical parameters), knowledge distillation (transferring knowledge from a large 'teacher' model to a smaller 'student' model), and quantization (decreasing parameter precision)."
18,"What is the main difference between Multi-Query Attention (MQA) and Grouped-Query Attention (GQA)?","In Multi-Query Attention (MQA), all attention heads share a single set of keys and values to reduce memory overhead. Grouped-Query Attention (GQA) is a middle ground where subgroups of query heads share a single key and value head, balancing performance and efficiency."
19,"The survey on SLMs describes how the 'rewrite-retrieve-read' framework uses a T5-large model. What specific role does the SLM play in this framework?","In the 'rewrite-retrieve-read' framework, an SLM (T5-large) acts as a 'Rewriter' to bridge the knowledge gap by rewriting user queries to be more effective for the retrieval component before the final 'read' or generation phase."
20,"LLM evaluation can be categorized into what three major groups?","The survey categorizes LLM evaluation into three major groups: 1) knowledge and capability evaluation, 2) alignment evaluation, and 3) safety evaluation."
21,"What are 'imitative falsehoods' as described by the TruthfulQA benchmark?","Imitative falsehoods are 'false statements that have a high likelihood under the model's training distribution'. The TruthfulQA benchmark is designed to test if models can avoid generating these falsehoods learned from their training data."
22,"What is the primary difference in how the StereoSet and CrowS-Pairs datasets measure stereotypical bias?","StereoSet measures bias by modifying attributes related to a target group (e.g., stereotypical vs. counter-stereotypical), while CrowS-Pairs measures bias by disrupting the groups themselves (e.g., comparing a sentence about a disadvantaged group to one about an advantaged group)."
23,"What is the core process of the QAQG-based method for evaluating factual consistency in summarization?","The QAQG method first uses a Question Generation (QG) model to create questions from the generated summary. Then, a Question Answering (QA) model answers those questions based on both the summary and the source document. The similarity between the two sets of answers is used to measure factual consistency."
24,"What is a consistently mentioned benefit of using Retrieval-Augmented Generation (RAG)?","All documents suggest RAG enhances model performance by integrating external knowledge. The RAG evaluation survey states it reduces hallucinations and improves reliability. The SLM survey notes it enables more informed and accurate outputs. The LLM evaluation survey mentions it as a technique for tool learning."
25,"HotpotQA is a benchmark for multi-hop reasoning. How do we use the HotpotQA dataset?","The 'Evaluating LLMs' survey identifies HotpotQA as a key dataset for multi-hop reasoning evaluation. The 'Evaluation of RAG' survey notes that evaluation frameworks like ARES use HotpotQA (as part of the KILT benchmark) as a source for constructing their own evaluation datasets."