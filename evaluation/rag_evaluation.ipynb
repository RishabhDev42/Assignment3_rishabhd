{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T16:19:09.530086Z",
     "start_time": "2025-10-14T16:18:19.659272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from pymilvus import MilvusClient, connections\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, answer_relevancy, context_precision\n",
    "from ragas import evaluate\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import dotenv"
   ],
   "id": "fd78d57e35bde97a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devgo\\Desktop\\NLXLLMs\\Assignment3_rishabhd\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Get RAG Evaluation Dataset",
   "id": "b32e66f34ac7cd97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T16:21:04.252989Z",
     "start_time": "2025-10-14T16:21:04.160781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csv_path = \"ragas_evaluation_dataset.csv\"\n",
    "queries = pd.read_csv(csv_path)\n",
    "print(queries.head())"
   ],
   "id": "f0d1c9481fad2cce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           question  \\\n",
      "0   1  What are the three main evaluation metrics use...   \n",
      "1   2  What type of model does ARES fine-tune to act ...   \n",
      "2   3  What is the role of Prediction-Powered Inferen...   \n",
      "3   4  Who introduced the concept of Knowledge Distil...   \n",
      "4   5   What is the main goal of Knowledge Distillation?   \n",
      "\n",
      "                                              answer  \n",
      "0  ARES evaluates RAG systems based on context re...  \n",
      "1  ARES fine-tunes DeBERTa-v3-Large models as LLM...  \n",
      "2  Prediction-Powered Inference provides confiden...  \n",
      "3  Knowledge Distillation was formally introduced...  \n",
      "4  The goal of Knowledge Distillation is to trans...  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load environment variables and dataset",
   "id": "b541296a54ce0d4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:49:03.845861Z",
     "start_time": "2025-10-14T17:49:00.142219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "# Prepare lists to store results\n",
    "generated_answers = []\n",
    "retrieved_contexts = []\n",
    "questions = []\n",
    "ground_truths = []\n",
    "\n",
    "# Initialize the embedding model and Milvus client\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "client = MilvusClient()\n",
    "connections.connect()\n",
    "\n",
    "# Initialize the LLM\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=google_api_key)\n",
    "gemini_model = genai.GenerativeModel('gemini-2.5-pro')"
   ],
   "id": "ac36f97203fff05a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run queries and store results",
   "id": "229ee292aaea97dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:57:45.291831Z",
     "start_time": "2025-10-14T17:52:07.718990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for index, row in tqdm(queries.iterrows(), total=len(queries)):\n",
    "    question = row['question']\n",
    "    ground_truth = row['answer']\n",
    "    query_embedding = embedding_model.embed_query(question)\n",
    "    search_results = client.search(\n",
    "        collection_name=\"learning_portal\",\n",
    "        data=[query_embedding],\n",
    "        limit=5,\n",
    "        output_fields=[\"passage\"]\n",
    "    )\n",
    "    context = \"\\n\".join([hit['entity']['passage'] for hit in search_results[0]])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        You are a personalized learning assistant. Your goal is to provide a clear and comprehensive answer to the user's question.\n",
    "\n",
    "        **User's Question:**\n",
    "        {question}\n",
    "\n",
    "        **Here is some context retrieved from the learning materials:**\n",
    "        <retrieved_context>\n",
    "        {context}\n",
    "        </retrieved_context>\n",
    "\n",
    "\n",
    "        **Instructions:**\n",
    "        1. Synthesize the information from the retrieved context, conversation history, and long-term memory to formulate your answer.\n",
    "        2. If the provided context is insufficient or the question requires very recent information, use your built-in Google Search tool to find the most up-to-date facts.\n",
    "        3. Provide a direct and helpful answer. Cite the source of your information if it comes from an external search.\n",
    "        \"\"\"\n",
    "    try:\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        answer = response.text\n",
    "    except Exception as e:\n",
    "        answer = f\"Error generating response: {e}\"\n",
    "    questions.append(question)\n",
    "    generated_answers.append(answer)\n",
    "    retrieved_contexts.append([context])  # Ragas expects list of lists\n",
    "    ground_truths.append(ground_truth)\n",
    "\n",
    "rag_results = pd.DataFrame({\n",
    "    \"question\": questions,\n",
    "    \"answer\": generated_answers,\n",
    "    \"contexts\": retrieved_contexts,\n",
    "    \"ground_truths\": ground_truths\n",
    "})\n",
    "\n",
    "data = {\n",
    "    \"user_input\": rag_results[\"question\"].tolist(),\n",
    "    \"response\": rag_results[\"answer\"].tolist(),\n",
    "    \"retrieved_contexts\": rag_results[\"contexts\"].tolist(),\n",
    "    \"reference\": rag_results[\"ground_truths\"].tolist()\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)"
   ],
   "id": "bb8abe72349ba967",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [05:37<00:00, 13.49s/it]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run RAGAS evaluation",
   "id": "dfb901a6a0ccbe56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:07:22.613957Z",
     "start_time": "2025-10-14T17:57:57.767844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "openai_api_key =  os.getenv(\"OPENAI_API_KEY\")\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "openai_llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key)\n",
    "evaluator_llm = LangchainLLMWrapper(openai_llm)\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[\n",
    "        Faithfulness(),\n",
    "        answer_relevancy,\n",
    "        LLMContextRecall(),\n",
    "        context_precision,\n",
    "        FactualCorrectness()\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "print(result)"
   ],
   "id": "793da691b462967c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devgo\\AppData\\Local\\Temp\\ipykernel_37696\\1068912840.py:4: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use the modern LLM providers instead: from ragas.llms.base import llm_factory; llm = llm_factory('gpt-4o-mini') or from ragas.llms.base import instructor_llm_factory; llm = instructor_llm_factory('openai', client=openai_client)\n",
      "  evaluator_llm = LangchainLLMWrapper(openai_llm)\n",
      "Evaluating:   0%|          | 0/125 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   1%|          | 1/125 [00:08<18:20,  8.87s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   8%|▊         | 10/125 [00:54<07:39,  4.00s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  13%|█▎        | 16/125 [01:39<10:19,  5.68s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  19%|█▉        | 24/125 [01:50<05:31,  3.28s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  31%|███       | 39/125 [02:47<04:07,  2.87s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  42%|████▏     | 53/125 [03:38<03:08,  2.62s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  43%|████▎     | 54/125 [03:48<04:14,  3.59s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  46%|████▌     | 57/125 [04:09<05:28,  4.83s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  56%|█████▌    | 70/125 [04:59<03:41,  4.03s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  67%|██████▋   | 84/125 [05:41<01:53,  2.76s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  68%|██████▊   | 85/125 [05:57<03:21,  5.03s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  74%|███████▎  | 92/125 [06:16<01:44,  3.18s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  74%|███████▍  | 93/125 [06:24<02:06,  3.97s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  83%|████████▎ | 104/125 [07:03<01:08,  3.27s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  84%|████████▍ | 105/125 [07:12<01:23,  4.18s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 125/125 [09:12<00:00,  4.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.5634, 'answer_relevancy': 0.9645, 'context_recall': 0.7567, 'context_precision': 0.8400, 'factual_correctness(mode=f1)': 0.5084}\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
